{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "547c464a-a973-4968-b327-1b3dfa83a067",
   "metadata": {},
   "source": [
    "## Import the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459192e1-27b5-4dbb-8cd8-e3b0b935b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19cd88d-361e-48cc-a54c-dfa0c4b5cf4f",
   "metadata": {},
   "source": [
    "## Load and Cleanup Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e25b0438-4975-4e67-a282-7067017900f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_data = pd.read_csv(\"COGS118A_FINAL/wdbc.data\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62ce038-05c4-43a0-a952-2b7376ebe44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"ID\", \"Diagnosis\", \"mean_radius\", \"mean_texture\", \"mean_perimeter\", \"mean_area\", \"mean_smoothness\", \n",
    "    \"mean_compactness\", \"mean_concavity\", \"mean_concave_points\", \"mean_symmetry\", \"mean_fractal_dimension\", \n",
    "    \"se_radius\", \"se_texture\", \"se_perimeter\", \"se_area\", \"se_smoothness\", \n",
    "    \"se_compactness\", \"se_concavity\", \"se_concave_points\", \"se_symmetry\", \"se_fractal_dimension\", \n",
    "    \"worst_radius\", \"worst_texture\", \"worst_perimeter\", \"worst_area\", \"worst_smoothness\", \n",
    "    \"worst_compactness\", \"worst_concavity\", \"worst_concave_points\", \"worst_symmetry\", \"worst_fractal_dimension\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef0d4aa-1911-48df-b2b4-a081c47e0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7db2c02-597b-49bb-9202-343353d83038",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "breast_cancer_data['Diagnosis'] = encoder.fit_transform(breast_cancer_data['Diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dadaa89a-342b-4bb7-bebd-0f0bc5e515ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_radius</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Diagnosis  mean_radius  mean_texture  mean_perimeter  mean_area  \\\n",
       "0    842302          1        17.99         10.38          122.80     1001.0   \n",
       "1    842517          1        20.57         17.77          132.90     1326.0   \n",
       "2  84300903          1        19.69         21.25          130.00     1203.0   \n",
       "3  84348301          1        11.42         20.38           77.58      386.1   \n",
       "4  84358402          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   mean_smoothness  mean_compactness  mean_concavity  mean_concave_points  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  worst_radius  worst_texture  worst_perimeter  worst_area  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   worst_smoothness  worst_compactness  worst_concavity  worst_concave_points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst_symmetry  worst_fractal_dimension  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c58e674-5e0f-4828-ace4-2fdc5038db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons_data = pd.read_csv(\"COGS118A_FINAL/parkinsons.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0993e4c1-aa6d-47d5-ab81-8025cae40538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>MDVP:Fo(Hz)</th>\n",
       "      <th>MDVP:Fhi(Hz)</th>\n",
       "      <th>MDVP:Flo(Hz)</th>\n",
       "      <th>MDVP:Jitter(%)</th>\n",
       "      <th>MDVP:Jitter(Abs)</th>\n",
       "      <th>MDVP:RAP</th>\n",
       "      <th>MDVP:PPQ</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>MDVP:Shimmer</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>status</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>spread1</th>\n",
       "      <th>spread2</th>\n",
       "      <th>D2</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phon_R01_S01_1</td>\n",
       "      <td>119.992</td>\n",
       "      <td>157.302</td>\n",
       "      <td>74.997</td>\n",
       "      <td>0.00784</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00370</td>\n",
       "      <td>0.00554</td>\n",
       "      <td>0.01109</td>\n",
       "      <td>0.04374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06545</td>\n",
       "      <td>0.02211</td>\n",
       "      <td>21.033</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414783</td>\n",
       "      <td>0.815285</td>\n",
       "      <td>-4.813031</td>\n",
       "      <td>0.266482</td>\n",
       "      <td>2.301442</td>\n",
       "      <td>0.284654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phon_R01_S01_2</td>\n",
       "      <td>122.400</td>\n",
       "      <td>148.650</td>\n",
       "      <td>113.819</td>\n",
       "      <td>0.00968</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.00696</td>\n",
       "      <td>0.01394</td>\n",
       "      <td>0.06134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09403</td>\n",
       "      <td>0.01929</td>\n",
       "      <td>19.085</td>\n",
       "      <td>1</td>\n",
       "      <td>0.458359</td>\n",
       "      <td>0.819521</td>\n",
       "      <td>-4.075192</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>2.486855</td>\n",
       "      <td>0.368674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phon_R01_S01_3</td>\n",
       "      <td>116.682</td>\n",
       "      <td>131.111</td>\n",
       "      <td>111.555</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00544</td>\n",
       "      <td>0.00781</td>\n",
       "      <td>0.01633</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08270</td>\n",
       "      <td>0.01309</td>\n",
       "      <td>20.651</td>\n",
       "      <td>1</td>\n",
       "      <td>0.429895</td>\n",
       "      <td>0.825288</td>\n",
       "      <td>-4.443179</td>\n",
       "      <td>0.311173</td>\n",
       "      <td>2.342259</td>\n",
       "      <td>0.332634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phon_R01_S01_4</td>\n",
       "      <td>116.676</td>\n",
       "      <td>137.871</td>\n",
       "      <td>111.366</td>\n",
       "      <td>0.00997</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00502</td>\n",
       "      <td>0.00698</td>\n",
       "      <td>0.01505</td>\n",
       "      <td>0.05492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08771</td>\n",
       "      <td>0.01353</td>\n",
       "      <td>20.644</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434969</td>\n",
       "      <td>0.819235</td>\n",
       "      <td>-4.117501</td>\n",
       "      <td>0.334147</td>\n",
       "      <td>2.405554</td>\n",
       "      <td>0.368975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phon_R01_S01_5</td>\n",
       "      <td>116.014</td>\n",
       "      <td>141.781</td>\n",
       "      <td>110.655</td>\n",
       "      <td>0.01284</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00655</td>\n",
       "      <td>0.00908</td>\n",
       "      <td>0.01966</td>\n",
       "      <td>0.06425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.01767</td>\n",
       "      <td>19.649</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417356</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>-3.747787</td>\n",
       "      <td>0.234513</td>\n",
       "      <td>2.332180</td>\n",
       "      <td>0.410335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "0  phon_R01_S01_1      119.992       157.302        74.997         0.00784   \n",
       "1  phon_R01_S01_2      122.400       148.650       113.819         0.00968   \n",
       "2  phon_R01_S01_3      116.682       131.111       111.555         0.01050   \n",
       "3  phon_R01_S01_4      116.676       137.871       111.366         0.00997   \n",
       "4  phon_R01_S01_5      116.014       141.781       110.655         0.01284   \n",
       "\n",
       "   MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  ...  \\\n",
       "0           0.00007   0.00370   0.00554     0.01109       0.04374  ...   \n",
       "1           0.00008   0.00465   0.00696     0.01394       0.06134  ...   \n",
       "2           0.00009   0.00544   0.00781     0.01633       0.05233  ...   \n",
       "3           0.00009   0.00502   0.00698     0.01505       0.05492  ...   \n",
       "4           0.00011   0.00655   0.00908     0.01966       0.06425  ...   \n",
       "\n",
       "   Shimmer:DDA      NHR     HNR  status      RPDE       DFA   spread1  \\\n",
       "0      0.06545  0.02211  21.033       1  0.414783  0.815285 -4.813031   \n",
       "1      0.09403  0.01929  19.085       1  0.458359  0.819521 -4.075192   \n",
       "2      0.08270  0.01309  20.651       1  0.429895  0.825288 -4.443179   \n",
       "3      0.08771  0.01353  20.644       1  0.434969  0.819235 -4.117501   \n",
       "4      0.10470  0.01767  19.649       1  0.417356  0.823484 -3.747787   \n",
       "\n",
       "    spread2        D2       PPE  \n",
       "0  0.266482  2.301442  0.284654  \n",
       "1  0.335590  2.486855  0.368674  \n",
       "2  0.311173  2.342259  0.332634  \n",
       "3  0.334147  2.405554  0.368975  \n",
       "4  0.234513  2.332180  0.410335  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parkinsons_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f280d9-0856-4d22-b259-a382fe11887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = pd.read_csv(\"COGS118A_FINAL/heart.dat\", sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37f43779-625b-42d1-acd7-e922c04337eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"age\", \"sex\", \"chest_pain_type\", \"resting_blood_pressure\", \"serum_cholestoral\",\n",
    "    \"fasting_blood_sugar\", \"resting_electrocardiographic\", \"max_heart_rate\",\n",
    "    \"angina\", \"oldpeak\", \"slope\", \"major_vessels\", \"thal\", \"heart_disease\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94a2cc56-8cce-486c-b01a-022de0bcc0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a174c7-3abf-4bf4-a0a3-f9ee11817c0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chest_pain_type</th>\n",
       "      <th>resting_blood_pressure</th>\n",
       "      <th>serum_cholestoral</th>\n",
       "      <th>fasting_blood_sugar</th>\n",
       "      <th>resting_electrocardiographic</th>\n",
       "      <th>max_heart_rate</th>\n",
       "      <th>angina</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>major_vessels</th>\n",
       "      <th>thal</th>\n",
       "      <th>heart_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  chest_pain_type  resting_blood_pressure  serum_cholestoral  \\\n",
       "0  70.0  1.0              4.0                   130.0              322.0   \n",
       "1  67.0  0.0              3.0                   115.0              564.0   \n",
       "2  57.0  1.0              2.0                   124.0              261.0   \n",
       "3  64.0  1.0              4.0                   128.0              263.0   \n",
       "4  74.0  0.0              2.0                   120.0              269.0   \n",
       "\n",
       "   fasting_blood_sugar  resting_electrocardiographic  max_heart_rate  angina  \\\n",
       "0                  0.0                           2.0           109.0     0.0   \n",
       "1                  0.0                           2.0           160.0     0.0   \n",
       "2                  0.0                           0.0           141.0     0.0   \n",
       "3                  0.0                           0.0           105.0     1.0   \n",
       "4                  0.0                           2.0           121.0     1.0   \n",
       "\n",
       "   oldpeak  slope  major_vessels  thal  heart_disease  \n",
       "0      2.4    2.0            3.0   3.0              2  \n",
       "1      1.6    2.0            0.0   7.0              1  \n",
       "2      0.3    1.0            0.0   7.0              2  \n",
       "3      0.2    2.0            1.0   7.0              1  \n",
       "4      0.2    1.0            1.0   3.0              1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92997fe8-23c1-4533-ae08-027f1919df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_surgery = pd.read_csv(\"COGS118A_FINAL/haberman.data\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad6cf58a-b480-45a5-89d0-e60f0b6fc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns2 = [\"age\", \"operation_year\", \"pos_auxillary_nodes\", \"survival_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14b60983-1506-474f-9fe7-3b18984d0954",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_surgery.columns = columns2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a029d17-52ae-46a8-8a9e-1ebe8770a25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>operation_year</th>\n",
       "      <th>pos_auxillary_nodes</th>\n",
       "      <th>survival_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  operation_year  pos_auxillary_nodes  survival_status\n",
       "0   30              64                    1                1\n",
       "1   30              62                    3                1\n",
       "2   30              65                    0                1\n",
       "3   31              59                    2                1\n",
       "4   31              65                    4                1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_surgery.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f42843-a9ca-4a7f-9396-03fd3947ff29",
   "metadata": {},
   "source": [
    "### The first dataset I will be working with is Breast Cancer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31061f-19d0-4ade-a568-b9da883b6d77",
   "metadata": {},
   "source": [
    "Below I will begin by partitioning the data and then I will train it to three classifiers Logistic Regression, Random Forest, and SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e44bf122-e151-4d9d-a0d4-2c1e07fe7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = breast_cancer_data.drop(columns=[\"ID\", \"Diagnosis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "553483a5-9e89-44c2-896a-e853702ef87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = breast_cancer_data[\"Diagnosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25342f0c-ee90-4cfd-bba4-350863826c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_20, X_test_80, y_train_20, y_test_80 = train_test_split(X, y, test_size=0.80, random_state=11) #20/80 split\n",
    "X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(X, y, test_size=0.50, random_state=11) #50/50 split\n",
    "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.20, random_state=11) #80/20 split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ee773-070b-45fc-a593-58ce25cc74f1",
   "metadata": {},
   "source": [
    "Here I will scale my data before applying the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04750174-deb4-4075-bb86-ae2dec1c36e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale Data\n",
    "scalar = StandardScaler()\n",
    "X_train_20_s = scalar.fit_transform(X_train_20)\n",
    "X_test_80_s = scalar.transform(X_test_80)\n",
    "\n",
    "X_train_50_s = scalar.fit_transform(X_train_50)\n",
    "X_test_50_s = scalar.transform(X_test_50)\n",
    "\n",
    "X_train_80_s = scalar.fit_transform(X_train_80)\n",
    "X_test_20_s = scalar.transform(X_test_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76b12e9e-4e36-4489-9e63-bbb60caa2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    #sklearn logistic regression\n",
    "    lr = LogisticRegression() \n",
    " \n",
    "    param_grid = {\n",
    "     \"penalty\" : [\"l2\"],\n",
    "     \"solver\" : [\"lbfgs\"],\n",
    "     \"C\": [1, 10, 100],\n",
    "     \"max_iter\": [100, 1000, 2000, 5000,10000]\n",
    "    }\n",
    "\n",
    "    # cross validation 5-folds\n",
    "    grid_search = GridSearchCV(lr, param_grid, cv=5, scoring=\"accuracy\", return_train_score=True) \n",
    "    best_grid_search = grid_search.fit(X_train, y_train)\n",
    "    best_lr = best_grid_search.best_estimator_\n",
    "    \n",
    "    train_accuracy = best_lr.score(X_train, y_train)\n",
    "    cross_validation = best_grid_search.best_score_\n",
    "    y_pred = best_lr.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "                    \n",
    "    return train_accuracy, test_accuracy, cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3d19088-af14-4c95-93ed-6a5c9ac8d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0    20/80        0.991150       0.962719                   0.973518\n",
      "1    50/50        0.991150       0.962719                   0.973518\n",
      "2    80/20        0.991150       0.962719                   0.973518\n",
      "3    20/80        0.996479       0.961404                   0.968358\n",
      "4    50/50        0.996479       0.961404                   0.968358\n",
      "5    80/20        0.996479       0.961404                   0.968358\n",
      "6    20/80        0.986813       0.982456                   0.975824\n",
      "7    50/50        0.986813       0.982456                   0.975824\n",
      "8    80/20        0.986813       0.982456                   0.975824\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.991481                           0.972567   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0                0.96886  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_lr1, test_accuracy1_lr1, cross_validation1_lr1 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_lr1, test_accuracy2_lr1, cross_validation2_lr1 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_lr1, test_accuracy3_lr1, cross_validation3_lr1 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_lr1, test_accuracy4_lr1, cross_validation4_lr1 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_lr1, test_accuracy5_lr1, cross_validation5_lr1 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_lr1, test_accuracy6_lr1, cross_validation6_lr1 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_lr1, test_accuracy7_lr1, cross_validation7_lr1 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_lr1, test_accuracy8_lr1, cross_validation8_lr1 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_lr1, test_accuracy9_lr1, cross_validation9_lr1 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_lr1 = (train_accuracy1_lr1 + train_accuracy2_lr1 + train_accuracy3_lr1 + train_accuracy4_lr1 + train_accuracy5_lr1 + train_accuracy6_lr1 + train_accuracy7_lr1 + train_accuracy8_lr1 +train_accuracy9_lr1) / 9\n",
    "\n",
    "cv_average_lr1 = (cross_validation1_lr1 + cross_validation2_lr1 + cross_validation3_lr1 + cross_validation4_lr1 + cross_validation5_lr1 + cross_validation6_lr1 + cross_validation7_lr1 + cross_validation8_lr1 + cross_validation9_lr1) / 9\n",
    "\n",
    "test_average_lr1 = (test_accuracy1_lr1 + test_accuracy2_lr1 + test_accuracy3_lr1 + test_accuracy4_lr1 + test_accuracy5_lr1 + test_accuracy6_lr1 + test_accuracy7_lr1 + test_accuracy8_lr1 + test_accuracy9_lr1) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Parition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_lr1, train_accuracy2_lr1, train_accuracy3_lr1, train_accuracy4_lr1, train_accuracy5_lr1, train_accuracy6_lr1, train_accuracy7_lr1, train_accuracy8_lr1, train_accuracy9_lr1],\n",
    "    \"Test Accuracy\": [test_accuracy1_lr1, test_accuracy2_lr1, test_accuracy3_lr1, test_accuracy4_lr1, test_accuracy5_lr1, test_accuracy6_lr1, test_accuracy7_lr1, test_accuracy8_lr1, test_accuracy9_lr1],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_lr1, cross_validation2_lr1, cross_validation3_lr1, cross_validation4_lr1, cross_validation5_lr1, cross_validation6_lr1, cross_validation7_lr1, cross_validation8_lr1, cross_validation9_lr1]\n",
    "})\n",
    "\n",
    "averages_lr1 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_lr1], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_lr1],\n",
    "    \"Test Accuracy Average\": [test_average_lr1]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_lr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223a1b71-f31d-4f28-943a-1b938517e67c",
   "metadata": {},
   "source": [
    "I will now apply the Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb859cc2-3104-4d07-9358-b1b988248948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    param_grid = {\"n_estimators\": [50, 100, 200],\n",
    "              \"max_depth\" : [1, 2, 3, 4, 5], \n",
    "              \"min_samples_split\" : [2, 4, 6, 10]\n",
    "             }\n",
    "    \n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\")\n",
    "    best_grid_search = grid_search.fit(X_train, y_train)\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    \n",
    "    train_accuracy = best_rf.score(X_train, y_train)\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    cross_validation = best_grid_search.best_score_\n",
    "\n",
    "    return train_accuracy, test_accuracy, cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2de841f-3088-42d4-b551-892a43b9a0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        1.000000       0.938596                   0.920158\n",
      "1     50/50        1.000000       0.942982                   0.928854\n",
      "2     80/20        0.991150       0.929825                   0.919763\n",
      "3     20/80        0.992958       0.964912                   0.954261\n",
      "4     50/50        0.996479       0.957895                   0.950752\n",
      "5     80/20        0.992958       0.954386                   0.954261\n",
      "6     20/80        0.991209       0.973684                   0.958242\n",
      "7     50/50        0.991209       0.982456                   0.960440\n",
      "8     80/20        0.991209       0.982456                   0.960440\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                 0.99413                           0.945241   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.958577  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_rf1, test_accuracy1_rf1, cross_validation1_rf1 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_rf1, test_accuracy2_rf1, cross_validation2_rf1 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_rf1, test_accuracy3_rf1, cross_validation3_rf1 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_rf1, test_accuracy4_rf1, cross_validation4_rf1 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_rf1, test_accuracy5_rf1, cross_validation5_rf1 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_rf1, test_accuracy6_rf1, cross_validation6_rf1 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_rf1, test_accuracy7_rf1, cross_validation7_rf1 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_rf1, test_accuracy8_rf1, cross_validation8_rf1 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_rf1, test_accuracy9_rf1, cross_validation9_rf1 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_rf1 = (train_accuracy1_rf1 + train_accuracy2_rf1 + train_accuracy3_rf1 + train_accuracy4_rf1 + train_accuracy5_rf1 + train_accuracy6_rf1 + train_accuracy7_rf1 + train_accuracy8_rf1 + train_accuracy9_rf1) / 9\n",
    "\n",
    "cv_average_rf1 = (cross_validation1_rf1 + cross_validation2_rf1 + cross_validation3_rf1 + cross_validation4_rf1 + cross_validation5_rf1 + cross_validation6_rf1 + cross_validation7_rf1 + cross_validation8_rf1 + cross_validation9_rf1) / 9\n",
    "\n",
    "test_average_rf1 = (test_accuracy1_rf1 + test_accuracy2_rf1 + test_accuracy3_rf1 + test_accuracy4_rf1 + test_accuracy5_rf1 + test_accuracy6_rf1 + test_accuracy7_rf1 + test_accuracy8_rf1 + test_accuracy9_rf1) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_rf1, train_accuracy2_rf1, train_accuracy3_rf1, train_accuracy4_rf1, train_accuracy5_rf1, train_accuracy6_rf1, train_accuracy7_rf1, train_accuracy8_rf1, train_accuracy9_rf1],\n",
    "    \"Test Accuracy\": [test_accuracy1_rf1, test_accuracy2_rf1, test_accuracy3_rf1, test_accuracy4_rf1, test_accuracy5_rf1, test_accuracy6_rf1, test_accuracy7_rf1, test_accuracy8_rf1, test_accuracy9_rf1],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_rf1, cross_validation2_rf1, cross_validation3_rf1, cross_validation4_rf1, cross_validation5_rf1, cross_validation6_rf1, cross_validation7_rf1, cross_validation8_rf1, cross_validation9_rf1]\n",
    "})\n",
    "\n",
    "averages_rf1 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_rf1], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_rf1],\n",
    "    \"Test Accuracy Average\": [test_average_rf1]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_rf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c62c0d-7620-4b5f-8d36-854fea8cb159",
   "metadata": {},
   "source": [
    "Now, I will perform cross-validation using Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cf519c0-bde9-4e63-ba49-d36f665ffdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X_train,y_train, X_test, y_test):\n",
    "    \n",
    "    svc = SVC()\n",
    "\n",
    "    param_grid = {\n",
    "    \"C\" : [1, 10, 100, 1000],\n",
    "    \"gamma\" : [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "    \"kernel\" : [\"linear\", \"poly\", \"rbf\"], \n",
    "    \"degree\" : [2]\n",
    "}\n",
    "\n",
    "    grid_search = GridSearchCV(svc, param_grid, cv=5, scoring=\"accuracy\")\n",
    "    best_grid_search = grid_search.fit(X_train, y_train)\n",
    "    best_svc = grid_search.best_estimator_\n",
    "\n",
    "    train_accuracy = best_svc.score(X_train, y_train)\n",
    "    y_pred = best_svc.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    cross_validation = best_grid_search.best_score_\n",
    "\n",
    "    return train_accuracy, test_accuracy, cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21ae2a86-e6d5-42e5-bb75-a78202479513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        1.000000       0.962719                   0.982213\n",
      "1     50/50        1.000000       0.962719                   0.982213\n",
      "2     80/20        1.000000       0.962719                   0.982213\n",
      "3     20/80        1.000000       0.961404                   0.975439\n",
      "4     50/50        1.000000       0.961404                   0.975439\n",
      "5     80/20        1.000000       0.961404                   0.975439\n",
      "6     20/80        0.986813       0.964912                   0.980220\n",
      "7     50/50        0.986813       0.964912                   0.980220\n",
      "8     80/20        0.986813       0.964912                   0.980220\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.995604                           0.979291   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.963012  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_svm1, test_accuracy1_svm1, cross_validation1_svm1 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_svm1, test_accuracy2_svm1, cross_validation2_svm1 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_svm1, test_accuracy3_svm1, cross_validation3_svm1 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_svm1, test_accuracy4_svm1, cross_validation4_svm1 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_svm1, test_accuracy5_svm1, cross_validation5_svm1 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_svm1, test_accuracy6_svm1, cross_validation6_svm1 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_svm1, test_accuracy7_svm1, cross_validation7_svm1 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_svm1, test_accuracy8_svm1, cross_validation8_svm1 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_svm1, test_accuracy9_svm1, cross_validation9_svm1 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_svm1 = (train_accuracy1_svm1 + train_accuracy2_svm1 + train_accuracy3_svm1 + train_accuracy4_svm1 + train_accuracy5_svm1 + train_accuracy6_svm1 + train_accuracy7_svm1 + train_accuracy8_svm1 + train_accuracy9_svm1) / 9\n",
    "\n",
    "cv_average_svm1 = (cross_validation1_svm1 + cross_validation2_svm1 + cross_validation3_svm1 + cross_validation4_svm1 + cross_validation5_svm1 + cross_validation6_svm1 + cross_validation7_svm1 + cross_validation8_svm1 + cross_validation9_svm1) / 9\n",
    "\n",
    "test_average_svm1 = (test_accuracy1_svm1 + test_accuracy2_svm1 + test_accuracy3_svm1 + test_accuracy4_svm1 + test_accuracy5_svm1 + test_accuracy6_svm1 + test_accuracy7_svm1 + test_accuracy8_svm1 + test_accuracy9_svm1) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_svm1, train_accuracy2_svm1, train_accuracy3_svm1, train_accuracy4_svm1, train_accuracy5_svm1, train_accuracy6_svm1, train_accuracy7_svm1, train_accuracy8_svm1, train_accuracy9_svm1],\n",
    "    \"Test Accuracy\": [test_accuracy1_svm1, test_accuracy2_svm1, test_accuracy3_svm1, test_accuracy4_svm1, test_accuracy5_svm1, test_accuracy6_svm1, test_accuracy7_svm1, test_accuracy8_svm1, test_accuracy9_svm1],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_svm1, cross_validation2_svm1, cross_validation3_svm1, cross_validation4_svm1, cross_validation5_svm1, cross_validation6_svm1, cross_validation7_svm1, cross_validation8_svm1, cross_validation9_svm1]\n",
    "})\n",
    "\n",
    "averages_svm1 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_svm1], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_svm1],\n",
    "    \"Test Accuracy Average\": [test_average_svm1]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_svm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6b6890c-3584-4394-9a1f-acebc5af5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train,y_train, X_test, y_test):\n",
    "    \n",
    "    dt = DecisionTreeClassifier()\n",
    "\n",
    "    param_grid = {\n",
    "        \"max_depth\" : [1, 2, 4]\n",
    "}\n",
    "\n",
    "    grid_search = GridSearchCV(dt, param_grid, cv=5, scoring=\"accuracy\")\n",
    "    best_grid_search = grid_search.fit(X_train, y_train)\n",
    "    best_dt = grid_search.best_estimator_\n",
    "\n",
    "    train_accuracy = best_dt.score(X_train, y_train)\n",
    "    y_pred = best_dt.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    cross_validation = best_grid_search.best_score_\n",
    "\n",
    "    return train_accuracy, test_accuracy, cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c71c1bf0-25c5-4750-8f20-16bee1814c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        1.000000       0.923246                   0.866798\n",
      "1     50/50        1.000000       0.921053                   0.866403\n",
      "2     80/20        1.000000       0.918860                   0.849012\n",
      "3     20/80        0.926056       0.908772                   0.908459\n",
      "4     50/50        0.926056       0.908772                   0.908459\n",
      "5     80/20        0.926056       0.908772                   0.908459\n",
      "6     20/80        0.980220       0.956140                   0.927473\n",
      "7     50/50        0.980220       0.956140                   0.927473\n",
      "8     80/20        0.980220       0.956140                   0.920879\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.968759                           0.898157   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.928655  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_dt1, test_accuracy1_dt1, cross_validation1_dt1 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_dt1, test_accuracy2_dt1, cross_validation2_dt1 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_dt1, test_accuracy3_dt1, cross_validation3_dt1 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_dt1, test_accuracy4_dt1, cross_validation4_dt1 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_dt1, test_accuracy5_dt1, cross_validation5_dt1 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_dt1, test_accuracy6_dt1, cross_validation6_dt1 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_dt1, test_accuracy7_dt1, cross_validation7_dt1 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_dt1, test_accuracy8_dt1, cross_validation8_dt1 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_dt1, test_accuracy9_dt1, cross_validation9_dt1 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_dt1 = (train_accuracy1_dt1 + train_accuracy2_dt1 + train_accuracy3_dt1 + train_accuracy4_dt1 + train_accuracy5_dt1 + train_accuracy6_dt1 + train_accuracy7_dt1 + train_accuracy8_dt1 + train_accuracy9_dt1) / 9\n",
    "\n",
    "cv_average_dt1 = (cross_validation1_dt1 + cross_validation2_dt1 + cross_validation3_dt1 + cross_validation4_dt1 + cross_validation5_dt1 + cross_validation6_dt1 + cross_validation7_dt1 + cross_validation8_dt1 + cross_validation9_dt1) / 9\n",
    "\n",
    "test_average_dt1 = (test_accuracy1_dt1 + test_accuracy2_dt1 + test_accuracy3_dt1 + test_accuracy4_dt1 + test_accuracy5_dt1 + test_accuracy6_dt1 + test_accuracy7_dt1 + test_accuracy8_dt1 + test_accuracy9_dt1) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_dt1, train_accuracy2_dt1, train_accuracy3_dt1, train_accuracy4_dt1, train_accuracy5_dt1, train_accuracy6_dt1, train_accuracy7_dt1, train_accuracy8_dt1, train_accuracy9_dt1],\n",
    "    \"Test Accuracy\": [test_accuracy1_dt1, test_accuracy2_dt1, test_accuracy3_dt1, test_accuracy4_dt1, test_accuracy5_dt1, test_accuracy6_dt1, test_accuracy7_dt1, test_accuracy8_dt1, test_accuracy9_dt1],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_dt1, cross_validation2_dt1, cross_validation3_dt1, cross_validation4_dt1, cross_validation5_dt1, cross_validation6_dt1, cross_validation7_dt1, cross_validation8_dt1, cross_validation9_dt1]\n",
    "})\n",
    "\n",
    "averages_dt1 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_dt1], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_dt1],\n",
    "    \"Test Accuracy Average\": [test_average_dt1]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_dt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6bfa56-1c65-42da-8d52-8ff8da1f1a3b",
   "metadata": {},
   "source": [
    "### Neural Nets: Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa0163f6-6258-43e1-a476-1cabe72acdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_perceptron(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    mlp = MLPClassifier()\n",
    "\n",
    "    param_grid = {\n",
    "        \"hidden_layer_sizes\" : [(100, 100, 100), (150, 150, 150), (200, 200, 200)]\n",
    "        }\n",
    "    \n",
    "    grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring=\"accuracy\")\n",
    "    best_grid_search = grid_search.fit(X_train, y_train)\n",
    "    best_mlp = grid_search.best_estimator_\n",
    "    \n",
    "    train_accuracy = best_mlp.score(X_train, y_train)\n",
    "    y_pred = best_mlp.predict(X_test)\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    cross_validation = best_grid_search.best_score_\n",
    "\n",
    "    return train_accuracy, test_accuracy, cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79173cd5-6f2e-4d75-929e-261a4fe12d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80             1.0       0.960526                   0.982609\n",
      "1     50/50             1.0       0.962719                   0.973518\n",
      "2     80/20             1.0       0.958333                   0.964427\n",
      "3     20/80             1.0       0.964912                   0.978947\n",
      "4     50/50             1.0       0.961404                   0.978885\n",
      "5     80/20             1.0       0.968421                   0.982456\n",
      "6     20/80             1.0       0.956140                   0.971429\n",
      "7     50/50             1.0       0.964912                   0.975824\n",
      "8     80/20             1.0       0.973684                   0.971429\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                     1.0                           0.975503   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0                0.96345  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_mlp1, test_accuracy1_mlp1, cross_validation1_mlp1 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_mlp1, test_accuracy2_mlp1, cross_validation2_mlp1 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_mlp1, test_accuracy3_mlp1, cross_validation3_mlp1 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_mlp1, test_accuracy4_mlp1, cross_validation4_mlp1 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_mlp1, test_accuracy5_mlp1, cross_validation5_mlp1 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_mlp1, test_accuracy6_mlp1, cross_validation6_mlp1 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_mlp1, test_accuracy7_mlp1, cross_validation7_mlp1 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_mlp1, test_accuracy8_mlp1, cross_validation8_mlp1 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_mlp1, test_accuracy9_mlp1, cross_validation9_mlp1 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_mlp1 = (train_accuracy1_mlp1 + train_accuracy2_mlp1 + train_accuracy3_mlp1 + train_accuracy4_mlp1 + train_accuracy5_mlp1 + train_accuracy6_mlp1 + train_accuracy7_mlp1 + train_accuracy8_mlp1 + train_accuracy9_mlp1) / 9\n",
    "\n",
    "cv_average_mlp1 = (cross_validation1_mlp1 + cross_validation2_mlp1 + cross_validation3_mlp1 + cross_validation4_mlp1 + cross_validation5_mlp1 + cross_validation6_mlp1 + cross_validation7_mlp1 + cross_validation8_mlp1 + cross_validation9_mlp1) / 9\n",
    "\n",
    "test_average_mlp1 = (test_accuracy1_mlp1 + test_accuracy2_mlp1 + test_accuracy3_mlp1 + test_accuracy4_mlp1 + test_accuracy5_mlp1 + test_accuracy6_mlp1 + test_accuracy7_mlp1 + test_accuracy8_mlp1 + test_accuracy9_mlp1) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_mlp1, train_accuracy2_mlp1, train_accuracy3_mlp1, train_accuracy4_mlp1, train_accuracy5_mlp1, train_accuracy6_mlp1, train_accuracy7_mlp1, train_accuracy8_mlp1, train_accuracy9_mlp1],\n",
    "    \"Test Accuracy\": [test_accuracy1_mlp1, test_accuracy2_mlp1, test_accuracy3_mlp1, test_accuracy4_mlp1, test_accuracy5_mlp1, test_accuracy6_mlp1, test_accuracy7_mlp1, test_accuracy8_mlp1, test_accuracy9_mlp1],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_mlp1, cross_validation2_mlp1, cross_validation3_mlp1, cross_validation4_mlp1, cross_validation5_mlp1, cross_validation6_mlp1, cross_validation7_mlp1, cross_validation8_mlp1, cross_validation9_mlp1]\n",
    "})\n",
    "\n",
    "averages_mlp1 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_mlp1], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_mlp1],\n",
    "    \"Test Accuracy Average\": [test_average_mlp1]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_mlp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6811e-89fa-44bf-b031-eb3650518c8e",
   "metadata": {},
   "source": [
    "### Now I will use the Parkinsons Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09a8dc19-92fb-44b0-a4aa-3507a3d6a5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>MDVP:Fo(Hz)</th>\n",
       "      <th>MDVP:Fhi(Hz)</th>\n",
       "      <th>MDVP:Flo(Hz)</th>\n",
       "      <th>MDVP:Jitter(%)</th>\n",
       "      <th>MDVP:Jitter(Abs)</th>\n",
       "      <th>MDVP:RAP</th>\n",
       "      <th>MDVP:PPQ</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>MDVP:Shimmer</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>status</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>spread1</th>\n",
       "      <th>spread2</th>\n",
       "      <th>D2</th>\n",
       "      <th>PPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phon_R01_S01_1</td>\n",
       "      <td>119.992</td>\n",
       "      <td>157.302</td>\n",
       "      <td>74.997</td>\n",
       "      <td>0.00784</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00370</td>\n",
       "      <td>0.00554</td>\n",
       "      <td>0.01109</td>\n",
       "      <td>0.04374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06545</td>\n",
       "      <td>0.02211</td>\n",
       "      <td>21.033</td>\n",
       "      <td>1</td>\n",
       "      <td>0.414783</td>\n",
       "      <td>0.815285</td>\n",
       "      <td>-4.813031</td>\n",
       "      <td>0.266482</td>\n",
       "      <td>2.301442</td>\n",
       "      <td>0.284654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phon_R01_S01_2</td>\n",
       "      <td>122.400</td>\n",
       "      <td>148.650</td>\n",
       "      <td>113.819</td>\n",
       "      <td>0.00968</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.00696</td>\n",
       "      <td>0.01394</td>\n",
       "      <td>0.06134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09403</td>\n",
       "      <td>0.01929</td>\n",
       "      <td>19.085</td>\n",
       "      <td>1</td>\n",
       "      <td>0.458359</td>\n",
       "      <td>0.819521</td>\n",
       "      <td>-4.075192</td>\n",
       "      <td>0.335590</td>\n",
       "      <td>2.486855</td>\n",
       "      <td>0.368674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phon_R01_S01_3</td>\n",
       "      <td>116.682</td>\n",
       "      <td>131.111</td>\n",
       "      <td>111.555</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00544</td>\n",
       "      <td>0.00781</td>\n",
       "      <td>0.01633</td>\n",
       "      <td>0.05233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08270</td>\n",
       "      <td>0.01309</td>\n",
       "      <td>20.651</td>\n",
       "      <td>1</td>\n",
       "      <td>0.429895</td>\n",
       "      <td>0.825288</td>\n",
       "      <td>-4.443179</td>\n",
       "      <td>0.311173</td>\n",
       "      <td>2.342259</td>\n",
       "      <td>0.332634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phon_R01_S01_4</td>\n",
       "      <td>116.676</td>\n",
       "      <td>137.871</td>\n",
       "      <td>111.366</td>\n",
       "      <td>0.00997</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00502</td>\n",
       "      <td>0.00698</td>\n",
       "      <td>0.01505</td>\n",
       "      <td>0.05492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08771</td>\n",
       "      <td>0.01353</td>\n",
       "      <td>20.644</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434969</td>\n",
       "      <td>0.819235</td>\n",
       "      <td>-4.117501</td>\n",
       "      <td>0.334147</td>\n",
       "      <td>2.405554</td>\n",
       "      <td>0.368975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phon_R01_S01_5</td>\n",
       "      <td>116.014</td>\n",
       "      <td>141.781</td>\n",
       "      <td>110.655</td>\n",
       "      <td>0.01284</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>0.00655</td>\n",
       "      <td>0.00908</td>\n",
       "      <td>0.01966</td>\n",
       "      <td>0.06425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10470</td>\n",
       "      <td>0.01767</td>\n",
       "      <td>19.649</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417356</td>\n",
       "      <td>0.823484</td>\n",
       "      <td>-3.747787</td>\n",
       "      <td>0.234513</td>\n",
       "      <td>2.332180</td>\n",
       "      <td>0.410335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
       "0  phon_R01_S01_1      119.992       157.302        74.997         0.00784   \n",
       "1  phon_R01_S01_2      122.400       148.650       113.819         0.00968   \n",
       "2  phon_R01_S01_3      116.682       131.111       111.555         0.01050   \n",
       "3  phon_R01_S01_4      116.676       137.871       111.366         0.00997   \n",
       "4  phon_R01_S01_5      116.014       141.781       110.655         0.01284   \n",
       "\n",
       "   MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  ...  \\\n",
       "0           0.00007   0.00370   0.00554     0.01109       0.04374  ...   \n",
       "1           0.00008   0.00465   0.00696     0.01394       0.06134  ...   \n",
       "2           0.00009   0.00544   0.00781     0.01633       0.05233  ...   \n",
       "3           0.00009   0.00502   0.00698     0.01505       0.05492  ...   \n",
       "4           0.00011   0.00655   0.00908     0.01966       0.06425  ...   \n",
       "\n",
       "   Shimmer:DDA      NHR     HNR  status      RPDE       DFA   spread1  \\\n",
       "0      0.06545  0.02211  21.033       1  0.414783  0.815285 -4.813031   \n",
       "1      0.09403  0.01929  19.085       1  0.458359  0.819521 -4.075192   \n",
       "2      0.08270  0.01309  20.651       1  0.429895  0.825288 -4.443179   \n",
       "3      0.08771  0.01353  20.644       1  0.434969  0.819235 -4.117501   \n",
       "4      0.10470  0.01767  19.649       1  0.417356  0.823484 -3.747787   \n",
       "\n",
       "    spread2        D2       PPE  \n",
       "0  0.266482  2.301442  0.284654  \n",
       "1  0.335590  2.486855  0.368674  \n",
       "2  0.311173  2.342259  0.332634  \n",
       "3  0.334147  2.405554  0.368975  \n",
       "4  0.234513  2.332180  0.410335  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parkinsons_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa783db7-4c12-4539-a2aa-f5c9a960eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = parkinsons_data.drop(columns=[\"name\", \"status\"])\n",
    "y = parkinsons_data[\"status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4909554-e0db-4a0f-aa76-5f7eaa477de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_20, X_test_80, y_train_20, y_test_80 = train_test_split(X, y, test_size=0.80, random_state=11) #20/80 split\n",
    "X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(X, y, test_size=0.50, random_state=11) #50/50 split\n",
    "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.20, random_state=11) #80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d0ad7b0-e3cb-4898-8088-7e015854ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale Data\n",
    "scalar = StandardScaler()\n",
    "X_train_20_s = scalar.fit_transform(X_train_20)\n",
    "X_test_80_s = scalar.transform(X_test_80)\n",
    "\n",
    "X_train_50_s = scalar.fit_transform(X_train_50)\n",
    "X_test_50_s = scalar.transform(X_test_50)\n",
    "\n",
    "X_train_80_s = scalar.fit_transform(X_train_80)\n",
    "X_test_20_s = scalar.transform(X_test_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c45cc-7627-49e2-a8af-53d8d42be844",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9313247-4cc4-4682-8dad-60276302012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.948718       0.846154                   0.871429\n",
      "1     50/50        0.948718       0.846154                   0.871429\n",
      "2     80/20        0.948718       0.846154                   0.871429\n",
      "3     20/80        0.876289       0.826531                   0.845263\n",
      "4     50/50        0.876289       0.826531                   0.845263\n",
      "5     80/20        0.876289       0.826531                   0.845263\n",
      "6     20/80        0.897436       0.769231                   0.826411\n",
      "7     50/50        0.897436       0.769231                   0.826411\n",
      "8     80/20        0.897436       0.769231                   0.826411\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.907481                           0.847701   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.813972  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_lr2, test_accuracy1_lr2, cross_validation1_lr2 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_lr2, test_accuracy2_lr2, cross_validation2_lr2 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_lr2, test_accuracy3_lr2, cross_validation3_lr2 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_lr2, test_accuracy4_lr2, cross_validation4_lr2 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_lr2, test_accuracy5_lr2, cross_validation5_lr2 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_lr2, test_accuracy6_lr2, cross_validation6_lr2 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_lr2, test_accuracy7_lr2, cross_validation7_lr2 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_lr2, test_accuracy8_lr2, cross_validation8_lr2 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_lr2, test_accuracy9_lr2, cross_validation9_lr2 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_lr2 = (train_accuracy1_lr2 + train_accuracy2_lr2 + train_accuracy3_lr2 + train_accuracy4_lr2 + train_accuracy5_lr2 + train_accuracy6_lr2 + train_accuracy7_lr2 + train_accuracy8_lr2 + train_accuracy9_lr2) / 9\n",
    "\n",
    "cv_average_lr2 = (cross_validation1_lr2 + cross_validation2_lr2 + cross_validation3_lr2 + cross_validation4_lr2 + cross_validation5_lr2 + cross_validation6_lr2 + cross_validation7_lr2 + cross_validation8_lr2 + cross_validation9_lr2) / 9\n",
    "\n",
    "test_average_lr2 = (test_accuracy1_lr2 + test_accuracy2_lr2 + test_accuracy3_lr2 + test_accuracy4_lr2 + test_accuracy5_lr2 + test_accuracy6_lr2 + test_accuracy7_lr2 + test_accuracy8_lr2 + test_accuracy9_lr2) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_lr2, train_accuracy2_lr2, train_accuracy3_lr2, train_accuracy4_lr2, train_accuracy5_lr2, train_accuracy6_lr2, train_accuracy7_lr2, train_accuracy8_lr2, train_accuracy9_lr2],\n",
    "    \"Test Accuracy\": [test_accuracy1_lr2, test_accuracy2_lr2, test_accuracy3_lr2, test_accuracy4_lr2, test_accuracy5_lr2, test_accuracy6_lr2, test_accuracy7_lr2, test_accuracy8_lr2, test_accuracy9_lr2],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_lr2, cross_validation2_lr2, cross_validation3_lr2, cross_validation4_lr2, cross_validation5_lr2, cross_validation6_lr2, cross_validation7_lr2, cross_validation8_lr2, cross_validation9_lr2]\n",
    "})\n",
    "\n",
    "averages_lr2 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_lr2], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_lr2],\n",
    "    \"Test Accuracy Average\": [test_average_lr2]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_lr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226fc42-7f85-4e90-b133-b89437892bab",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b455b81-d08e-4027-8cf6-8ed0d4a4faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.871795       0.846154                   0.871429\n",
      "1     50/50        0.897436       0.852564                   0.871429\n",
      "2     80/20        0.974359       0.858974                   0.871429\n",
      "3     20/80        0.927835       0.867347                   0.865263\n",
      "4     50/50        0.917526       0.867347                   0.875263\n",
      "5     80/20        0.979381       0.918367                   0.865263\n",
      "6     20/80        0.974359       0.897436                   0.903629\n",
      "7     50/50        1.000000       0.948718                   0.909879\n",
      "8     80/20        1.000000       0.897436                   0.916532\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.949188                           0.883346   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.883816  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_rf2, test_accuracy1_rf2, cross_validation1_rf2 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_rf2, test_accuracy2_rf2, cross_validation2_rf2 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_rf2, test_accuracy3_rf2, cross_validation3_rf2 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_rf2, test_accuracy4_rf2, cross_validation4_rf2 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_rf2, test_accuracy5_rf2, cross_validation5_rf2 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_rf2, test_accuracy6_rf2, cross_validation6_rf2 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_rf2, test_accuracy7_rf2, cross_validation7_rf2 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_rf2, test_accuracy8_rf2, cross_validation8_rf2 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_rf2, test_accuracy9_rf2, cross_validation9_rf2 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_rf2 = (train_accuracy1_rf2 + train_accuracy2_rf2 + train_accuracy3_rf2 + train_accuracy4_rf2 + train_accuracy5_rf2 + train_accuracy6_rf2 + train_accuracy7_rf2 + train_accuracy8_rf2 + train_accuracy9_rf2) / 9\n",
    "\n",
    "cv_average_rf2 = (cross_validation1_rf2 + cross_validation2_rf2 + cross_validation3_rf2 + cross_validation4_rf2 + cross_validation5_rf2 + cross_validation6_rf2 + cross_validation7_rf2 + cross_validation8_rf2 + cross_validation9_rf2) / 9\n",
    "\n",
    "test_average_rf2 = (test_accuracy1_rf2 + test_accuracy2_rf2 + test_accuracy3_rf2 + test_accuracy4_rf2 + test_accuracy5_rf2 + test_accuracy6_rf2 + test_accuracy7_rf2 + test_accuracy8_rf2 + test_accuracy9_rf2) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_rf2, train_accuracy2_rf2, train_accuracy3_rf2, train_accuracy4_rf2, train_accuracy5_rf2, train_accuracy6_rf2, train_accuracy7_rf2, train_accuracy8_rf2, train_accuracy9_rf2],\n",
    "    \"Test Accuracy\": [test_accuracy1_rf2, test_accuracy2_rf2, test_accuracy3_rf2, test_accuracy4_rf2, test_accuracy5_rf2, test_accuracy6_rf2, test_accuracy7_rf2, test_accuracy8_rf2, test_accuracy9_rf2],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_rf2, cross_validation2_rf2, cross_validation3_rf2, cross_validation4_rf2, cross_validation5_rf2, cross_validation6_rf2, cross_validation7_rf2, cross_validation8_rf2, cross_validation9_rf2]\n",
    "})\n",
    "\n",
    "averages_rf2 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_rf2], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_rf2],\n",
    "    \"Test Accuracy Average\": [test_average_rf2]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_rf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2af5ee-c7ac-473f-b8f3-b73952218d59",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2494cf2a-e0cb-4bd8-b339-809980b56257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.897436       0.871795                   0.896429\n",
      "1     50/50        0.897436       0.871795                   0.896429\n",
      "2     80/20        0.897436       0.871795                   0.896429\n",
      "3     20/80        0.989691       0.928571                   0.896316\n",
      "4     50/50        0.989691       0.928571                   0.896316\n",
      "5     80/20        0.989691       0.928571                   0.896316\n",
      "6     20/80        1.000000       1.000000                   0.929032\n",
      "7     50/50        1.000000       1.000000                   0.929032\n",
      "8     80/20        1.000000       1.000000                   0.929032\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.962376                           0.907259   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.933455  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_svm2, test_accuracy1_svm2, cross_validation1_svm2 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_svm2, test_accuracy2_svm2, cross_validation2_svm2 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_svm2, test_accuracy3_svm2, cross_validation3_svm2 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_svm2, test_accuracy4_svm2, cross_validation4_svm2 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_svm2, test_accuracy5_svm2, cross_validation5_svm2 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_svm2, test_accuracy6_svm2, cross_validation6_svm2 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_svm2, test_accuracy7_svm2, cross_validation7_svm2 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_svm2, test_accuracy8_svm2, cross_validation8_svm2 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_svm2, test_accuracy9_svm2, cross_validation9_svm2 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_svm2 = (train_accuracy1_svm2 + train_accuracy2_svm2 + train_accuracy3_svm2 + train_accuracy4_svm2 + train_accuracy5_svm2 + train_accuracy6_svm2 + train_accuracy7_svm2 + train_accuracy8_svm2 + train_accuracy9_svm2) / 9\n",
    "\n",
    "cv_average_svm2 = (cross_validation1_svm2 + cross_validation2_svm2 + cross_validation3_svm2 + cross_validation4_svm2 + cross_validation5_svm2 + cross_validation6_svm2 + cross_validation7_svm2 + cross_validation8_svm2 + cross_validation9_svm2) / 9\n",
    "\n",
    "test_average_svm2 = (test_accuracy1_svm2 + test_accuracy2_svm2 + test_accuracy3_svm2 + test_accuracy4_svm2 + test_accuracy5_svm2 + test_accuracy6_svm2 + test_accuracy7_svm2 + test_accuracy8_svm2 + test_accuracy9_svm2) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_svm2, train_accuracy2_svm2, train_accuracy3_svm2, train_accuracy4_svm2, train_accuracy5_svm2, train_accuracy6_svm2, train_accuracy7_svm2, train_accuracy8_svm2, train_accuracy9_svm2],\n",
    "    \"Test Accuracy\": [test_accuracy1_svm2, test_accuracy2_svm2, test_accuracy3_svm2, test_accuracy4_svm2, test_accuracy5_svm2, test_accuracy6_svm2, test_accuracy7_svm2, test_accuracy8_svm2, test_accuracy9_svm2],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_svm2, cross_validation2_svm2, cross_validation3_svm2, cross_validation4_svm2, cross_validation5_svm2, cross_validation6_svm2, cross_validation7_svm2, cross_validation8_svm2, cross_validation9_svm2]\n",
    "})\n",
    "\n",
    "averages_svm2 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_svm2], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_svm2],\n",
    "    \"Test Accuracy Average\": [test_average_svm2]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_svm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c6b77-dfda-456f-b43e-ee82e5e8567e",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea6df9b8-3e89-4b1f-947f-2ba366537356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        1.000000       0.858974                   0.775000\n",
      "1     50/50        0.923077       0.826923                   0.714286\n",
      "2     80/20        0.923077       0.826923                   0.771429\n",
      "3     20/80        0.855670       0.877551                   0.803684\n",
      "4     50/50        0.855670       0.877551                   0.803684\n",
      "5     80/20        0.855670       0.877551                   0.793158\n",
      "6     20/80        0.993590       0.871795                   0.871573\n",
      "7     50/50        0.871795       0.846154                   0.858669\n",
      "8     80/20        0.993590       0.871795                   0.871573\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.919127                           0.807006   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.859469  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_dt2, test_accuracy1_dt2, cross_validation1_dt2 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_dt2, test_accuracy2_dt2, cross_validation2_dt2 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_dt2, test_accuracy3_dt2, cross_validation3_dt2 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_dt2, test_accuracy4_dt2, cross_validation4_dt2 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_dt2, test_accuracy5_dt2, cross_validation5_dt2 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_dt2, test_accuracy6_dt2, cross_validation6_dt2 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_dt2, test_accuracy7_dt2, cross_validation7_dt2 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_dt2, test_accuracy8_dt2, cross_validation8_dt2 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_dt2, test_accuracy9_dt2, cross_validation9_dt2 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_dt2 = (train_accuracy1_dt2 + train_accuracy2_dt2 + train_accuracy3_dt2 + train_accuracy4_dt2 + train_accuracy5_dt2 + train_accuracy6_dt2 + train_accuracy7_dt2 + train_accuracy8_dt2 + train_accuracy9_dt2) / 9\n",
    "\n",
    "cv_average_dt2 = (cross_validation1_dt2 + cross_validation2_dt2 + cross_validation3_dt2 + cross_validation4_dt2 + cross_validation5_dt2 + cross_validation6_dt2 + cross_validation7_dt2 + cross_validation8_dt2 + cross_validation9_dt2) / 9\n",
    "\n",
    "test_average_dt2 = (test_accuracy1_dt2 + test_accuracy2_dt2 + test_accuracy3_dt2 + test_accuracy4_dt2 + test_accuracy5_dt2 + test_accuracy6_dt2 + test_accuracy7_dt2 + test_accuracy8_dt2 + test_accuracy9_dt2) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_dt2, train_accuracy2_dt2, train_accuracy3_dt2, train_accuracy4_dt2, train_accuracy5_dt2, train_accuracy6_dt2, train_accuracy7_dt2, train_accuracy8_dt2, train_accuracy9_dt2],\n",
    "    \"Test Accuracy\": [test_accuracy1_dt2, test_accuracy2_dt2, test_accuracy3_dt2, test_accuracy4_dt2, test_accuracy5_dt2, test_accuracy6_dt2, test_accuracy7_dt2, test_accuracy8_dt2, test_accuracy9_dt2],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_dt2, cross_validation2_dt2, cross_validation3_dt2, cross_validation4_dt2, cross_validation5_dt2, cross_validation6_dt2, cross_validation7_dt2, cross_validation8_dt2, cross_validation9_dt2]\n",
    "})\n",
    "\n",
    "averages_dt2 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_dt2], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_dt2],\n",
    "    \"Test Accuracy Average\": [test_average_dt2]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_dt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd1e97-5034-4620-bf8d-e5ae980a5b0e",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "948908f1-9638-44df-b72f-981e7245adbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80             1.0       0.865385                   0.850000\n",
      "1     50/50             1.0       0.814103                   0.850000\n",
      "2     80/20             1.0       0.865385                   0.846429\n",
      "3     20/80             1.0       0.887755                   0.897368\n",
      "4     50/50             1.0       0.877551                   0.897368\n",
      "5     80/20             1.0       0.897959                   0.907368\n",
      "6     20/80             1.0       0.923077                   0.929234\n",
      "7     50/50             1.0       0.923077                   0.935484\n",
      "8     80/20             1.0       0.923077                   0.929032\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                     1.0                           0.893587   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.886374  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_mlp2, test_accuracy1_mlp2, cross_validation1_mlp2 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_mlp2, test_accuracy2_mlp2, cross_validation2_mlp2 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_mlp2, test_accuracy3_mlp2, cross_validation3_mlp2 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_mlp2, test_accuracy4_mlp2, cross_validation4_mlp2 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_mlp2, test_accuracy5_mlp2, cross_validation5_mlp2 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_mlp2, test_accuracy6_mlp2, cross_validation6_mlp2 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_mlp2, test_accuracy7_mlp2, cross_validation7_mlp2 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_mlp2, test_accuracy8_mlp2, cross_validation8_mlp2 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_mlp2, test_accuracy9_mlp2, cross_validation9_mlp2 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_mlp2 = (train_accuracy1_mlp2 + train_accuracy2_mlp2 + train_accuracy3_mlp2 + train_accuracy4_mlp2 + train_accuracy5_mlp2 + train_accuracy6_mlp2 + train_accuracy7_mlp2 + train_accuracy8_mlp2 + train_accuracy9_mlp2) / 9\n",
    "\n",
    "cv_average_mlp2 = (cross_validation1_mlp2 + cross_validation2_mlp2 + cross_validation3_mlp2 + cross_validation4_mlp2 + cross_validation5_mlp2 + cross_validation6_mlp2 + cross_validation7_mlp2 + cross_validation8_mlp2 + cross_validation9_mlp2) / 9\n",
    "\n",
    "test_average_mlp2 = (test_accuracy1_mlp2 + test_accuracy2_mlp2 + test_accuracy3_mlp2 + test_accuracy4_mlp2 + test_accuracy5_mlp2 + test_accuracy6_mlp2 + test_accuracy7_mlp2 + test_accuracy8_mlp2 + test_accuracy9_mlp2) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_mlp2, train_accuracy2_mlp2, train_accuracy3_mlp2, train_accuracy4_mlp2, train_accuracy5_mlp2, train_accuracy6_mlp2, train_accuracy7_mlp2, train_accuracy8_mlp2, train_accuracy9_mlp2],\n",
    "    \"Test Accuracy\": [test_accuracy1_mlp2, test_accuracy2_mlp2, test_accuracy3_mlp2, test_accuracy4_mlp2, test_accuracy5_mlp2, test_accuracy6_mlp2, test_accuracy7_mlp2, test_accuracy8_mlp2, test_accuracy9_mlp2],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_mlp2, cross_validation2_mlp2, cross_validation3_mlp2, cross_validation4_mlp2, cross_validation5_mlp2, cross_validation6_mlp2, cross_validation7_mlp2, cross_validation8_mlp2, cross_validation9_mlp2]\n",
    "})\n",
    "\n",
    "averages_mlp2 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_mlp2], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_mlp2],\n",
    "    \"Test Accuracy Average\": [test_average_mlp2]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_mlp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afbfad6-c076-4494-9863-93b8c3a81578",
   "metadata": {},
   "source": [
    "### Now I will use the Heart Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "115ce520-12f8-446b-aca1-202677c70955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chest_pain_type</th>\n",
       "      <th>resting_blood_pressure</th>\n",
       "      <th>serum_cholestoral</th>\n",
       "      <th>fasting_blood_sugar</th>\n",
       "      <th>resting_electrocardiographic</th>\n",
       "      <th>max_heart_rate</th>\n",
       "      <th>angina</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>major_vessels</th>\n",
       "      <th>thal</th>\n",
       "      <th>heart_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  chest_pain_type  resting_blood_pressure  serum_cholestoral  \\\n",
       "0  70.0  1.0              4.0                   130.0              322.0   \n",
       "1  67.0  0.0              3.0                   115.0              564.0   \n",
       "2  57.0  1.0              2.0                   124.0              261.0   \n",
       "3  64.0  1.0              4.0                   128.0              263.0   \n",
       "4  74.0  0.0              2.0                   120.0              269.0   \n",
       "\n",
       "   fasting_blood_sugar  resting_electrocardiographic  max_heart_rate  angina  \\\n",
       "0                  0.0                           2.0           109.0     0.0   \n",
       "1                  0.0                           2.0           160.0     0.0   \n",
       "2                  0.0                           0.0           141.0     0.0   \n",
       "3                  0.0                           0.0           105.0     1.0   \n",
       "4                  0.0                           2.0           121.0     1.0   \n",
       "\n",
       "   oldpeak  slope  major_vessels  thal  heart_disease  \n",
       "0      2.4    2.0            3.0   3.0              2  \n",
       "1      1.6    2.0            0.0   7.0              1  \n",
       "2      0.3    1.0            0.0   7.0              2  \n",
       "3      0.2    2.0            1.0   7.0              1  \n",
       "4      0.2    1.0            1.0   3.0              1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1ec04fe-642b-4e5b-9628-bdb26fd1c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_data.drop(columns=[\"heart_disease\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbe0e005-95a0-4992-9d15-91e1de9acc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = heart_data[\"heart_disease\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba8da009-b01f-4608-9e7f-a30c859912de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_20, X_test_80, y_train_20, y_test_80 = train_test_split(X, y, test_size=0.80, random_state=11) #20/80 split\n",
    "X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(X, y, test_size=0.50, random_state=11) #50/50 split\n",
    "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.20, random_state=11) #80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d70b4663-cf39-4f2a-9788-70e2ab322a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale Data\n",
    "scalar = StandardScaler()\n",
    "X_train_20_s = scalar.fit_transform(X_train_20)\n",
    "X_test_80_s = scalar.transform(X_test_80)\n",
    "\n",
    "X_train_50_s = scalar.fit_transform(X_train_50)\n",
    "X_test_50_s = scalar.transform(X_test_50)\n",
    "\n",
    "X_train_80_s = scalar.fit_transform(X_train_80)\n",
    "X_test_20_s = scalar.transform(X_test_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a69dea-1aef-43ce-9ba4-6b5ed67a0635",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50a240e9-8f6b-487a-8dc6-bc61fb2dad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.925926       0.759259                   0.778182\n",
      "1     50/50        0.925926       0.759259                   0.778182\n",
      "2     80/20        0.925926       0.759259                   0.778182\n",
      "3     20/80        0.844444       0.807407                   0.844444\n",
      "4     50/50        0.844444       0.807407                   0.844444\n",
      "5     80/20        0.844444       0.807407                   0.844444\n",
      "6     20/80        0.847222       0.870370                   0.819450\n",
      "7     50/50        0.847222       0.870370                   0.819450\n",
      "8     80/20        0.847222       0.870370                   0.819450\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.872531                           0.814026   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.812346  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_lr3, test_accuracy1_lr3, cross_validation1_lr3 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_lr3, test_accuracy2_lr3, cross_validation2_lr3 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_lr3, test_accuracy3_lr3, cross_validation3_lr3 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_lr3, test_accuracy4_lr3, cross_validation4_lr3 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_lr3, test_accuracy5_lr3, cross_validation5_lr3 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_lr3, test_accuracy6_lr3, cross_validation6_lr3 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_lr3, test_accuracy7_lr3, cross_validation7_lr3 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_lr3, test_accuracy8_lr3, cross_validation8_lr3 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_lr3, test_accuracy9_lr3, cross_validation9_lr3 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_lr3 = (train_accuracy1_lr3 + train_accuracy2_lr3 + train_accuracy3_lr3 + train_accuracy4_lr3 + train_accuracy5_lr3 + train_accuracy6_lr3 + train_accuracy7_lr3 + train_accuracy8_lr3 + train_accuracy9_lr3) / 9\n",
    "\n",
    "cv_average_lr3 = (cross_validation1_lr3 + cross_validation2_lr3 + cross_validation3_lr3 + cross_validation4_lr3 + cross_validation5_lr3 + cross_validation6_lr3 + cross_validation7_lr3 + cross_validation8_lr3 + cross_validation9_lr3) / 9\n",
    "\n",
    "test_average_lr3 = (test_accuracy1_lr3 + test_accuracy2_lr3 + test_accuracy3_lr3 + test_accuracy4_lr3 + test_accuracy5_lr3 + test_accuracy6_lr3 + test_accuracy7_lr3 + test_accuracy8_lr3 + test_accuracy9_lr3) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_lr3, train_accuracy2_lr3, train_accuracy3_lr3, train_accuracy4_lr3, train_accuracy5_lr3, train_accuracy6_lr3, train_accuracy7_lr3, train_accuracy8_lr3, train_accuracy9_lr3],\n",
    "    \"Test Accuracy\": [test_accuracy1_lr3, test_accuracy2_lr3, test_accuracy3_lr3, test_accuracy4_lr3, test_accuracy5_lr3, test_accuracy6_lr3, test_accuracy7_lr3, test_accuracy8_lr3, test_accuracy9_lr3],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_lr3, cross_validation2_lr3, cross_validation3_lr3, cross_validation4_lr3, cross_validation5_lr3, cross_validation6_lr3, cross_validation7_lr3, cross_validation8_lr3, cross_validation9_lr3]\n",
    "})\n",
    "\n",
    "averages_lr3 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_lr3], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_lr3],\n",
    "    \"Test Accuracy Average\": [test_average_lr3]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_lr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee9c09-9c43-49f4-9071-0121cfe57d91",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03a90890-7dcd-468b-a489-accdeb86c738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.981481       0.763889                   0.870909\n",
      "1     50/50        0.944444       0.754630                   0.870909\n",
      "2     80/20        0.944444       0.745370                   0.870909\n",
      "3     20/80        0.874074       0.837037                   0.851852\n",
      "4     50/50        0.866667       0.792593                   0.851852\n",
      "5     80/20        0.851852       0.822222                   0.851852\n",
      "6     20/80        0.875000       0.888889                   0.851797\n",
      "7     50/50        0.851852       0.907407                   0.856342\n",
      "8     80/20        0.912037       0.851852                   0.856342\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.900206                           0.859196   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0                0.81821  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_rf3, test_accuracy1_rf3, cross_validation1_rf3 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_rf3, test_accuracy2_rf3, cross_validation2_rf3 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_rf3, test_accuracy3_rf3, cross_validation3_rf3 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_rf3, test_accuracy4_rf3, cross_validation4_rf3 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_rf3, test_accuracy5_rf3, cross_validation5_rf3 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_rf3, test_accuracy6_rf3, cross_validation6_rf3 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_rf3, test_accuracy7_rf3, cross_validation7_rf3 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_rf3, test_accuracy8_rf3, cross_validation8_rf3 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_rf3, test_accuracy9_rf3, cross_validation9_rf3 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_rf3 = (train_accuracy1_rf3 + train_accuracy2_rf3 + train_accuracy3_rf3 + train_accuracy4_rf3 + train_accuracy5_rf3 + train_accuracy6_rf3 + train_accuracy7_rf3 + train_accuracy8_rf3 + train_accuracy9_rf3) / 9\n",
    "\n",
    "cv_average_rf3 = (cross_validation1_rf3 + cross_validation2_rf3 + cross_validation3_rf3 + cross_validation4_rf3 + cross_validation5_rf3 + cross_validation6_rf3 + cross_validation7_rf3 + cross_validation8_rf3 + cross_validation9_rf3) / 9\n",
    "\n",
    "test_average_rf3 = (test_accuracy1_rf3 + test_accuracy2_rf3 + test_accuracy3_rf3 + test_accuracy4_rf3 + test_accuracy5_rf3 + test_accuracy6_rf3 + test_accuracy7_rf3 + test_accuracy8_rf3 + test_accuracy9_rf3) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_rf3, train_accuracy2_rf3, train_accuracy3_rf3, train_accuracy4_rf3, train_accuracy5_rf3, train_accuracy6_rf3, train_accuracy7_rf3, train_accuracy8_rf3, train_accuracy9_rf3],\n",
    "    \"Test Accuracy\": [test_accuracy1_rf3, test_accuracy2_rf3, test_accuracy3_rf3, test_accuracy4_rf3, test_accuracy5_rf3, test_accuracy6_rf3, test_accuracy7_rf3, test_accuracy8_rf3, test_accuracy9_rf3],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_rf3, cross_validation2_rf3, cross_validation3_rf3, cross_validation4_rf3, cross_validation5_rf3, cross_validation6_rf3, cross_validation7_rf3, cross_validation8_rf3, cross_validation9_rf3]\n",
    "})\n",
    "\n",
    "averages_rf3 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_rf3], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_rf3],\n",
    "    \"Test Accuracy Average\": [test_average_rf3]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_rf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e854741-63c0-4b96-8f89-59007ec53208",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14a86181-52b4-4737-a625-a8584e70a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.907407       0.800926                   0.870909\n",
      "1     50/50        0.907407       0.800926                   0.870909\n",
      "2     80/20        0.907407       0.800926                   0.870909\n",
      "3     20/80        0.866667       0.851852                   0.844444\n",
      "4     50/50        0.866667       0.851852                   0.844444\n",
      "5     80/20        0.866667       0.851852                   0.844444\n",
      "6     20/80        0.851852       0.870370                   0.832981\n",
      "7     50/50        0.851852       0.870370                   0.832981\n",
      "8     80/20        0.851852       0.870370                   0.832981\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.875309                           0.849445   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.841049  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_svm3, test_accuracy1_svm3, cross_validation1_svm3 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_svm3, test_accuracy2_svm3, cross_validation2_svm3 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_svm3, test_accuracy3_svm3, cross_validation3_svm3 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_svm3, test_accuracy4_svm3, cross_validation4_svm3 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_svm3, test_accuracy5_svm3, cross_validation5_svm3 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_svm3, test_accuracy6_svm3, cross_validation6_svm3 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_svm3, test_accuracy7_svm3, cross_validation7_svm3 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_svm3, test_accuracy8_svm3, cross_validation8_svm3 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_svm3, test_accuracy9_svm3, cross_validation9_svm3 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_svm3 = (train_accuracy1_svm3 + train_accuracy2_svm3 + train_accuracy3_svm3 + train_accuracy4_svm3 + train_accuracy5_svm3 + train_accuracy6_svm3 + train_accuracy7_svm3 + train_accuracy8_svm3 + train_accuracy9_svm3) / 9\n",
    "\n",
    "cv_average_svm3 = (cross_validation1_svm3 + cross_validation2_svm3 + cross_validation3_svm3 + cross_validation4_svm3 + cross_validation5_svm3 + cross_validation6_svm3 + cross_validation7_svm3 + cross_validation8_svm3 + cross_validation9_svm3) / 9\n",
    "\n",
    "test_average_svm3 = (test_accuracy1_svm3 + test_accuracy2_svm3 + test_accuracy3_svm3 + test_accuracy4_svm3 + test_accuracy5_svm3 + test_accuracy6_svm3 + test_accuracy7_svm3 + test_accuracy8_svm3 + test_accuracy9_svm3) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_svm3, train_accuracy2_svm3, train_accuracy3_svm3, train_accuracy4_svm3, train_accuracy5_svm3, train_accuracy6_svm3, train_accuracy7_svm3, train_accuracy8_svm3, train_accuracy9_svm3],\n",
    "    \"Test Accuracy\": [test_accuracy1_svm3, test_accuracy2_svm3, test_accuracy3_svm3, test_accuracy4_svm3, test_accuracy5_svm3, test_accuracy6_svm3, test_accuracy7_svm3, test_accuracy8_svm3, test_accuracy9_svm3],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_svm3, cross_validation2_svm3, cross_validation3_svm3, cross_validation4_svm3, cross_validation5_svm3, cross_validation6_svm3, cross_validation7_svm3, cross_validation8_svm3, cross_validation9_svm3]\n",
    "})\n",
    "\n",
    "averages_svm3 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_svm3], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_svm3],\n",
    "    \"Test Accuracy Average\": [test_average_svm3]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_svm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c120fa-b7b0-4289-be61-a2b37cef850e",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e01a5d4-8a44-4b91-a64d-da172e02a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.907407       0.671296                   0.718182\n",
      "1     50/50        0.907407       0.671296                   0.718182\n",
      "2     80/20        0.907407       0.671296                   0.718182\n",
      "3     20/80        0.896296       0.829630                   0.674074\n",
      "4     50/50        0.755556       0.770370                   0.674074\n",
      "5     80/20        0.755556       0.770370                   0.674074\n",
      "6     20/80        0.902778       0.833333                   0.814693\n",
      "7     50/50        0.902778       0.814815                   0.814693\n",
      "8     80/20        0.902778       0.833333                   0.814693\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.870885                            0.73565   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0                0.76286  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_dt3, test_accuracy1_dt3, cross_validation1_dt3 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_dt3, test_accuracy2_dt3, cross_validation2_dt3 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_dt3, test_accuracy3_dt3, cross_validation3_dt3 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_dt3, test_accuracy4_dt3, cross_validation4_dt3 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_dt3, test_accuracy5_dt3, cross_validation5_dt3 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_dt3, test_accuracy6_dt3, cross_validation6_dt3 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_dt3, test_accuracy7_dt3, cross_validation7_dt3 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_dt3, test_accuracy8_dt3, cross_validation8_dt3 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_dt3, test_accuracy9_dt3, cross_validation9_dt3 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_dt3 = (train_accuracy1_dt3 + train_accuracy2_dt3 + train_accuracy3_dt3 + train_accuracy4_dt3 + train_accuracy5_dt3 + train_accuracy6_dt3 + train_accuracy7_dt3 + train_accuracy8_dt3 + train_accuracy9_dt3) / 9\n",
    "\n",
    "cv_average_dt3 = (cross_validation1_dt3 + cross_validation2_dt3 + cross_validation3_dt3 + cross_validation4_dt3 + cross_validation5_dt3 + cross_validation6_dt3 + cross_validation7_dt3 + cross_validation8_dt3 + cross_validation9_dt3) / 9\n",
    "\n",
    "test_average_dt3 = (test_accuracy1_dt3 + test_accuracy2_dt3 + test_accuracy3_dt3 + test_accuracy4_dt3 + test_accuracy5_dt3 + test_accuracy6_dt3 + test_accuracy7_dt3 + test_accuracy8_dt3 + test_accuracy9_dt3) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_dt3, train_accuracy2_dt3, train_accuracy3_dt3, train_accuracy4_dt3, train_accuracy5_dt3, train_accuracy6_dt3, train_accuracy7_dt3, train_accuracy8_dt3, train_accuracy9_dt3],\n",
    "    \"Test Accuracy\": [test_accuracy1_dt3, test_accuracy2_dt3, test_accuracy3_dt3, test_accuracy4_dt3, test_accuracy5_dt3, test_accuracy6_dt3, test_accuracy7_dt3, test_accuracy8_dt3, test_accuracy9_dt3],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_dt3, cross_validation2_dt3, cross_validation3_dt3, cross_validation4_dt3, cross_validation5_dt3, cross_validation6_dt3, cross_validation7_dt3, cross_validation8_dt3, cross_validation9_dt3]\n",
    "})\n",
    "\n",
    "averages_dt3 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_dt3], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_dt3],\n",
    "    \"Test Accuracy Average\": [test_average_dt3]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_dt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1b838-f960-41ef-a6fe-0ce0d8ee1637",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e454438-a8db-40bc-a407-625ae658f0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80             1.0       0.750000                   0.834545\n",
      "1     50/50             1.0       0.763889                   0.852727\n",
      "2     80/20             1.0       0.754630                   0.852727\n",
      "3     20/80             1.0       0.807407                   0.814815\n",
      "4     50/50             1.0       0.800000                   0.800000\n",
      "5     80/20             1.0       0.807407                   0.800000\n",
      "6     20/80             1.0       0.814815                   0.800634\n",
      "7     50/50             1.0       0.796296                   0.796195\n",
      "8     80/20             1.0       0.851852                   0.791649\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                     1.0                           0.815921   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.794033  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_mlp3, test_accuracy1_mlp3, cross_validation1_mlp3 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_mlp3, test_accuracy2_mlp3, cross_validation2_mlp3 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_mlp3, test_accuracy3_mlp3, cross_validation3_mlp3 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_mlp3, test_accuracy4_mlp3, cross_validation4_mlp3 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_mlp3, test_accuracy5_mlp3, cross_validation5_mlp3 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_mlp3, test_accuracy6_mlp3, cross_validation6_mlp3 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_mlp3, test_accuracy7_mlp3, cross_validation7_mlp3 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_mlp3, test_accuracy8_mlp3, cross_validation8_mlp3 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_mlp3, test_accuracy9_mlp3, cross_validation9_mlp3 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_mlp3 = (train_accuracy1_mlp3 + train_accuracy2_mlp3 + train_accuracy3_mlp3 + train_accuracy4_mlp3 + train_accuracy5_mlp3 + train_accuracy6_mlp3 + train_accuracy7_mlp3 + train_accuracy8_mlp3 + train_accuracy9_mlp3) / 9\n",
    "\n",
    "cv_average_mlp3 = (cross_validation1_mlp3 + cross_validation2_mlp3 + cross_validation3_mlp3 + cross_validation4_mlp3 + cross_validation5_mlp3 + cross_validation6_mlp3 + cross_validation7_mlp3 + cross_validation8_mlp3 + cross_validation9_mlp3) / 9\n",
    "\n",
    "test_average_mlp3 = (test_accuracy1_mlp3 + test_accuracy2_mlp3 + test_accuracy3_mlp3 + test_accuracy4_mlp3 + test_accuracy5_mlp3 + test_accuracy6_mlp3 + test_accuracy7_mlp3 + test_accuracy8_mlp3 + test_accuracy9_mlp3) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_mlp3, train_accuracy2_mlp3, train_accuracy3_mlp3, train_accuracy4_mlp3, train_accuracy5_mlp3, train_accuracy6_mlp3, train_accuracy7_mlp3, train_accuracy8_mlp3, train_accuracy9_mlp3],\n",
    "    \"Test Accuracy\": [test_accuracy1_mlp3, test_accuracy2_mlp3, test_accuracy3_mlp3, test_accuracy4_mlp3, test_accuracy5_mlp3, test_accuracy6_mlp3, test_accuracy7_mlp3, test_accuracy8_mlp3, test_accuracy9_mlp3],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_mlp3, cross_validation2_mlp3, cross_validation3_mlp3, cross_validation4_mlp3, cross_validation5_mlp3, cross_validation6_mlp3, cross_validation7_mlp3, cross_validation8_mlp3, cross_validation9_mlp3]\n",
    "})\n",
    "\n",
    "averages_mlp3 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_mlp3], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_mlp3],\n",
    "    \"Test Accuracy Average\": [test_average_mlp3]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_mlp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2db098-424b-4199-bda8-b735404c4519",
   "metadata": {},
   "source": [
    "## Now I will use the Breast Cancer Survival Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de88cedd-34c7-4760-bb8f-072dbaa19ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>operation_year</th>\n",
       "      <th>pos_auxillary_nodes</th>\n",
       "      <th>survival_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  operation_year  pos_auxillary_nodes  survival_status\n",
       "0   30              64                    1                1\n",
       "1   30              62                    3                1\n",
       "2   30              65                    0                1\n",
       "3   31              59                    2                1\n",
       "4   31              65                    4                1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_surgery.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7dceea66-7ca6-4573-84a4-68c9ce5425be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer_surgery.drop(columns=\"survival_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "297969bd-3735-4d27-8e6c-eb7b981a705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cancer_surgery[\"survival_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9b8100fc-4788-4112-8355-ee38a6534096",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_20, X_test_80, y_train_20, y_test_80 = train_test_split(X, y, test_size=0.80, random_state=11) #20/80 split\n",
    "X_train_50, X_test_50, y_train_50, y_test_50 = train_test_split(X, y, test_size=0.50, random_state=11) #50/50 split\n",
    "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.20, random_state=11) #80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f8c76076-cbf3-4b87-86df-7afc0f49c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale Data\n",
    "scalar = StandardScaler()\n",
    "X_train_20_s = scalar.fit_transform(X_train_20)\n",
    "X_test_80_s = scalar.transform(X_test_80)\n",
    "\n",
    "X_train_50_s = scalar.fit_transform(X_train_50)\n",
    "X_test_50_s = scalar.transform(X_test_50)\n",
    "\n",
    "X_train_80_s = scalar.fit_transform(X_train_80)\n",
    "X_test_20_s = scalar.transform(X_test_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5cc21d-068b-4a95-9a77-f08f9fdff1e7",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ef63c942-9b83-4ee4-89de-a6033d3d6565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.721311        0.75102                   0.719231\n",
      "1     50/50        0.721311        0.75102                   0.719231\n",
      "2     80/20        0.721311        0.75102                   0.719231\n",
      "3     20/80        0.732026        0.75817                   0.725806\n",
      "4     50/50        0.732026        0.75817                   0.725806\n",
      "5     80/20        0.732026        0.75817                   0.725806\n",
      "6     20/80        0.729508        0.83871                   0.729592\n",
      "7     50/50        0.729508        0.83871                   0.729592\n",
      "8     80/20        0.729508        0.83871                   0.729592\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.727615                           0.724876   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.782633  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_lr4, test_accuracy1_lr4, cross_validation1_lr4 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_lr4, test_accuracy2_lr4, cross_validation2_lr4 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_lr4, test_accuracy3_lr4, cross_validation3_lr4 = logistic_regression(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_lr4, test_accuracy4_lr4, cross_validation4_lr4 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_lr4, test_accuracy5_lr4, cross_validation5_lr4 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_lr4, test_accuracy6_lr4, cross_validation6_lr4 = logistic_regression(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_lr4, test_accuracy7_lr4, cross_validation7_lr4 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_lr4, test_accuracy8_lr4, cross_validation8_lr4 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_lr4, test_accuracy9_lr4, cross_validation9_lr4 = logistic_regression(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_lr4 = (train_accuracy1_lr4 + train_accuracy2_lr4 + train_accuracy3_lr4 + train_accuracy4_lr4 + train_accuracy5_lr4 + train_accuracy6_lr4 + train_accuracy7_lr4 + train_accuracy8_lr4 + train_accuracy9_lr4) / 9\n",
    "\n",
    "cv_average_lr4 = (cross_validation1_lr4 + cross_validation2_lr4 + cross_validation3_lr4 + cross_validation4_lr4 + cross_validation5_lr4 + cross_validation6_lr4 + cross_validation7_lr4 + cross_validation8_lr4 + cross_validation9_lr4) / 9\n",
    "\n",
    "test_average_lr4 = (test_accuracy1_lr4 + test_accuracy2_lr4 + test_accuracy3_lr4 + test_accuracy4_lr4 + test_accuracy5_lr4 + test_accuracy6_lr4 + test_accuracy7_lr4 + test_accuracy8_lr4 + test_accuracy9_lr4) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_lr4, train_accuracy2_lr4, train_accuracy3_lr4, train_accuracy4_lr4, train_accuracy5_lr4, train_accuracy6_lr4, train_accuracy7_lr4, train_accuracy8_lr4, train_accuracy9_lr4],\n",
    "    \"Test Accuracy\": [test_accuracy1_lr4, test_accuracy2_lr4, test_accuracy3_lr4, test_accuracy4_lr4, test_accuracy5_lr4, test_accuracy6_lr4, test_accuracy7_lr4, test_accuracy8_lr4, test_accuracy9_lr4],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_lr4, cross_validation2_lr4, cross_validation3_lr4, cross_validation4_lr4, cross_validation5_lr4, cross_validation6_lr4, cross_validation7_lr4, cross_validation8_lr4, cross_validation9_lr4]\n",
    "})\n",
    "\n",
    "averages_lr4 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_lr4], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_lr4],\n",
    "    \"Test Accuracy Average\": [test_average_lr4]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_lr4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1a07a-27e1-4939-9012-199660956163",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b011c995-958d-4205-9685-67e18c3a1cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.770492       0.771429                   0.706410\n",
      "1     50/50        0.786885       0.742857                   0.689744\n",
      "2     80/20        0.819672       0.742857                   0.706410\n",
      "3     20/80        0.797386       0.764706                   0.745376\n",
      "4     50/50        0.810458       0.764706                   0.752043\n",
      "5     80/20        0.797386       0.764706                   0.752043\n",
      "6     20/80        0.782787       0.838710                   0.729507\n",
      "7     50/50        0.795082       0.822581                   0.737670\n",
      "8     80/20        0.807377       0.806452                   0.733588\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.796392                           0.728088   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.779889  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_rf4, test_accuracy1_rf4, cross_validation1_rf4 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_rf4, test_accuracy2_rf4, cross_validation2_rf4 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_rf4, test_accuracy3_rf4, cross_validation3_rf4 = random_forest(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_rf4, test_accuracy4_rf4, cross_validation4_rf4 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_rf4, test_accuracy5_rf4, cross_validation5_rf4 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_rf4, test_accuracy6_rf4, cross_validation6_rf4 = random_forest(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_rf4, test_accuracy7_rf4, cross_validation7_rf4 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_rf4, test_accuracy8_rf4, cross_validation8_rf4 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_rf4, test_accuracy9_rf4, cross_validation9_rf4 = random_forest(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_rf4 = (train_accuracy1_rf4 + train_accuracy2_rf4 + train_accuracy3_rf4 + train_accuracy4_rf4 + train_accuracy5_rf4 + train_accuracy6_rf4 + train_accuracy7_rf4 + train_accuracy8_rf4 + train_accuracy9_rf4) / 9\n",
    "\n",
    "cv_average_rf4 = (cross_validation1_rf4 + cross_validation2_rf4 + cross_validation3_rf4 + cross_validation4_rf4 + cross_validation5_rf4 + cross_validation6_rf4 + cross_validation7_rf4 + cross_validation8_rf4 + cross_validation9_rf4) / 9\n",
    "\n",
    "test_average_rf4 = (test_accuracy1_rf4 + test_accuracy2_rf4 + test_accuracy3_rf4 + test_accuracy4_rf4 + test_accuracy5_rf4 + test_accuracy6_rf4 + test_accuracy7_rf4 + test_accuracy8_rf4 + test_accuracy9_rf4) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_rf4, train_accuracy2_rf4, train_accuracy3_rf4, train_accuracy4_rf4, train_accuracy5_rf4, train_accuracy6_rf4, train_accuracy7_rf4, train_accuracy8_rf4, train_accuracy9_rf4],\n",
    "    \"Test Accuracy\": [test_accuracy1_rf4, test_accuracy2_rf4, test_accuracy3_rf4, test_accuracy4_rf4, test_accuracy5_rf4, test_accuracy6_rf4, test_accuracy7_rf4, test_accuracy8_rf4, test_accuracy9_rf4],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_rf4, cross_validation2_rf4, cross_validation3_rf4, cross_validation4_rf4, cross_validation5_rf4, cross_validation6_rf4, cross_validation7_rf4, cross_validation8_rf4, cross_validation9_rf4]\n",
    "})\n",
    "\n",
    "averages_rf4 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_rf4], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_rf4],\n",
    "    \"Test Accuracy Average\": [test_average_rf4]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_rf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f5efe-f36b-4f66-bd5c-d87de2e9619b",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e02d4525-e190-40f9-b32b-749d71bf5711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.704918       0.759184                   0.703846\n",
      "1     50/50        0.704918       0.759184                   0.703846\n",
      "2     80/20        0.704918       0.759184                   0.703846\n",
      "3     20/80        0.751634       0.751634                   0.738710\n",
      "4     50/50        0.751634       0.751634                   0.738710\n",
      "5     80/20        0.751634       0.751634                   0.738710\n",
      "6     20/80        0.778689       0.822581                   0.729337\n",
      "7     50/50        0.778689       0.822581                   0.729337\n",
      "8     80/20        0.778689       0.822581                   0.729337\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                 0.74508                           0.723964   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.777799  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_svm4, test_accuracy1_svm4, cross_validation1_svm4 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_svm4, test_accuracy2_svm4, cross_validation2_svm4 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_svm4, test_accuracy3_svm4, cross_validation3_svm4 = svm(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_svm4, test_accuracy4_svm4, cross_validation4_svm4 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_svm4, test_accuracy5_svm4, cross_validation5_svm4 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_svm4, test_accuracy6_svm4, cross_validation6_svm4 = svm(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_svm4, test_accuracy7_svm4, cross_validation7_svm4 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_svm4, test_accuracy8_svm4, cross_validation8_svm4 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_svm4, test_accuracy9_svm4, cross_validation9_svm4 = svm(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_svm4 = (train_accuracy1_svm4 + train_accuracy2_svm4 + train_accuracy3_svm4 + train_accuracy4_svm4 + train_accuracy5_svm4 + train_accuracy6_svm4 + train_accuracy7_svm4 + train_accuracy8_svm4 + train_accuracy9_svm4) / 9\n",
    "\n",
    "cv_average_svm4 = (cross_validation1_svm4 + cross_validation2_svm4 + cross_validation3_svm4 + cross_validation4_svm4 + cross_validation5_svm4 + cross_validation6_svm4 + cross_validation7_svm4 + cross_validation8_svm4 + cross_validation9_svm4) / 9\n",
    "\n",
    "test_average_svm4 = (test_accuracy1_svm4 + test_accuracy2_svm4 + test_accuracy3_svm4 + test_accuracy4_svm4 + test_accuracy5_svm4 + test_accuracy6_svm4 + test_accuracy7_svm4 + test_accuracy8_svm4 + test_accuracy9_svm4) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_svm4, train_accuracy2_svm4, train_accuracy3_svm4, train_accuracy4_svm4, train_accuracy5_svm4, train_accuracy6_svm4, train_accuracy7_svm4, train_accuracy8_svm4, train_accuracy9_svm4],\n",
    "    \"Test Accuracy\": [test_accuracy1_svm4, test_accuracy2_svm4, test_accuracy3_svm4, test_accuracy4_svm4, test_accuracy5_svm4, test_accuracy6_svm4, test_accuracy7_svm4, test_accuracy8_svm4, test_accuracy9_svm4],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_svm4, cross_validation2_svm4, cross_validation3_svm4, cross_validation4_svm4, cross_validation5_svm4, cross_validation6_svm4, cross_validation7_svm4, cross_validation8_svm4, cross_validation9_svm4]\n",
    "})\n",
    "\n",
    "averages_svm4 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_svm4], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_svm4],\n",
    "    \"Test Accuracy Average\": [test_average_svm4]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_svm4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4160c-f91b-483e-9aaa-0cc3f9ec5058",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "61d4a240-7a8e-4c75-880f-b30cd81f56fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.704918       0.771429                   0.673077\n",
      "1     50/50        0.704918       0.771429                   0.673077\n",
      "2     80/20        0.704918       0.771429                   0.673077\n",
      "3     20/80        0.764706       0.732026                   0.732043\n",
      "4     50/50        0.764706       0.732026                   0.732043\n",
      "5     80/20        0.764706       0.732026                   0.732043\n",
      "6     20/80        0.774590       0.790323                   0.753997\n",
      "7     50/50        0.774590       0.790323                   0.753997\n",
      "8     80/20        0.774590       0.790323                   0.753997\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.748071                           0.719706   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0               0.764592  \n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_dt4, test_accuracy1_dt4, cross_validation1_dt4 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_dt4, test_accuracy2_dt4, cross_validation2_dt4 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_dt4, test_accuracy3_dt4, cross_validation3_dt4 = decision_tree(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_dt4, test_accuracy4_dt4, cross_validation4_dt4 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_dt4, test_accuracy5_dt4, cross_validation5_dt4 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_dt4, test_accuracy6_dt4, cross_validation6_dt4 = decision_tree(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_dt4, test_accuracy7_dt4, cross_validation7_dt4 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_dt4, test_accuracy8_dt4, cross_validation8_dt4 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_dt4, test_accuracy9_dt4, cross_validation9_dt4 = decision_tree(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_dt4 = (train_accuracy1_dt4 + train_accuracy2_dt4 + train_accuracy3_dt4 + train_accuracy4_dt4 + train_accuracy5_dt4 + train_accuracy6_dt4 + train_accuracy7_dt4 + train_accuracy8_dt4 + train_accuracy9_dt4) / 9\n",
    "\n",
    "cv_average_dt4 = (cross_validation1_dt4 + cross_validation2_dt4 + cross_validation3_dt4 + cross_validation4_dt4 + cross_validation5_dt4 + cross_validation6_dt4 + cross_validation7_dt4 + cross_validation8_dt4 + cross_validation9_dt4) / 9\n",
    "\n",
    "test_average_dt4 = (test_accuracy1_dt4 + test_accuracy2_dt4 + test_accuracy3_dt4 + test_accuracy4_dt4 + test_accuracy5_dt4 + test_accuracy6_dt4 + test_accuracy7_dt4 + test_accuracy8_dt4 + test_accuracy9_dt4) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_dt4, train_accuracy2_dt4, train_accuracy3_dt4, train_accuracy4_dt4, train_accuracy5_dt4, train_accuracy6_dt4, train_accuracy7_dt4, train_accuracy8_dt4, train_accuracy9_dt4],\n",
    "    \"Test Accuracy\": [test_accuracy1_dt4, test_accuracy2_dt4, test_accuracy3_dt4, test_accuracy4_dt4, test_accuracy5_dt4, test_accuracy6_dt4, test_accuracy7_dt4, test_accuracy8_dt4, test_accuracy9_dt4],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_dt4, cross_validation2_dt4, cross_validation3_dt4, cross_validation4_dt4, cross_validation5_dt4, cross_validation6_dt4, cross_validation7_dt4, cross_validation8_dt4, cross_validation9_dt4]\n",
    "})\n",
    "\n",
    "averages_dt4 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_dt4], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_dt4],\n",
    "    \"Test Accuracy Average\": [test_average_dt4]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_dt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279e675-3da4-4418-81d1-69825ba7c9de",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "480647f9-7273-4c33-a567-71793974ac0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Partition  Train Accuracy  Test Accuracy  Cross Validation Accuracy\n",
      "0     20/80        0.983607       0.571429                   0.639744\n",
      "1     50/50        0.983607       0.563265                   0.638462\n",
      "2     80/20        0.983607       0.575510                   0.655128\n",
      "3     20/80        0.986928       0.712418                   0.607097\n",
      "4     50/50        0.915033       0.692810                   0.627527\n",
      "5     80/20        0.986928       0.692810                   0.607527\n",
      "6     20/80        0.795082       0.822581                   0.643452\n",
      "7     50/50        0.848361       0.806452                   0.630952\n",
      "8     80/20        0.799180       0.822581                   0.635204\n",
      "   Train Accuracy Average  Cross Validation Accuracy Average  \\\n",
      "0                0.920259                           0.631677   \n",
      "\n",
      "   Test Accuracy Average  \n",
      "0                0.69554  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vale_\\anaconda3\\anaconda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_accuracy1_mlp4, test_accuracy1_mlp4, cross_validation1_mlp4 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy2_mlp4, test_accuracy2_mlp4, cross_validation2_mlp4 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy3_mlp4, test_accuracy3_mlp4, cross_validation3_mlp4 = multi_layer_perceptron(X_train_20_s, y_train_20, X_test_80_s, y_test_80)\n",
    "\n",
    "train_accuracy4_mlp4, test_accuracy4_mlp4, cross_validation4_mlp4 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy5_mlp4, test_accuracy5_mlp4, cross_validation5_mlp4 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy6_mlp4, test_accuracy6_mlp4, cross_validation6_mlp4 = multi_layer_perceptron(X_train_50_s, y_train_50, X_test_50_s, y_test_50)\n",
    "\n",
    "train_accuracy7_mlp4, test_accuracy7_mlp4, cross_validation7_mlp4 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy8_mlp4, test_accuracy8_mlp4, cross_validation8_mlp4 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "train_accuracy9_mlp4, test_accuracy9_mlp4, cross_validation9_mlp4 = multi_layer_perceptron(X_train_80_s, y_train_80, X_test_20_s, y_test_20)\n",
    "\n",
    "# Averages across all splits\n",
    "train_average_mlp4 = (train_accuracy1_mlp4 + train_accuracy2_mlp4 + train_accuracy3_mlp4 + train_accuracy4_mlp4 + train_accuracy5_mlp4 + train_accuracy6_mlp4 + train_accuracy7_mlp4 + train_accuracy8_mlp4 + train_accuracy9_mlp4) / 9\n",
    "\n",
    "cv_average_mlp4 = (cross_validation1_mlp4 + cross_validation2_mlp4 + cross_validation3_mlp4 + cross_validation4_mlp4 + cross_validation5_mlp4 + cross_validation6_mlp4 + cross_validation7_mlp4 + cross_validation8_mlp4 + cross_validation9_mlp4) / 9\n",
    "\n",
    "test_average_mlp4 = (test_accuracy1_mlp4 + test_accuracy2_mlp4 + test_accuracy3_mlp4 + test_accuracy4_mlp4 + test_accuracy5_mlp4 + test_accuracy6_mlp4 + test_accuracy7_mlp4 + test_accuracy8_mlp4 + test_accuracy9_mlp4) / 9\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Partition\" : [\"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\", \"20/80\", \"50/50\", \"80/20\"], \n",
    "    \"Train Accuracy\": [train_accuracy1_mlp4, train_accuracy2_mlp4, train_accuracy3_mlp4, train_accuracy4_mlp4, train_accuracy5_mlp4, train_accuracy6_mlp4, train_accuracy7_mlp4, train_accuracy8_mlp4, train_accuracy9_mlp4],\n",
    "    \"Test Accuracy\": [test_accuracy1_mlp4, test_accuracy2_mlp4, test_accuracy3_mlp4, test_accuracy4_mlp4, test_accuracy5_mlp4, test_accuracy6_mlp4, test_accuracy7_mlp4, test_accuracy8_mlp4, test_accuracy9_mlp4],\n",
    "    \"Cross Validation Accuracy\": [cross_validation1_mlp4, cross_validation2_mlp4, cross_validation3_mlp4, cross_validation4_mlp4, cross_validation5_mlp4, cross_validation6_mlp4, cross_validation7_mlp4, cross_validation8_mlp4, cross_validation9_mlp4]\n",
    "})\n",
    "\n",
    "averages_mlp4 = pd.DataFrame({\n",
    "    \"Train Accuracy Average\": [train_average_mlp4], \n",
    "    \"Cross Validation Accuracy Average\": [cv_average_mlp4],\n",
    "    \"Test Accuracy Average\": [test_average_mlp4]\n",
    "})\n",
    "\n",
    "print(results)\n",
    "print(averages_mlp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd31f49f-8461-433c-a728-aa17395c7ce6",
   "metadata": {},
   "source": [
    "## Results Across All Classifiers and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe11d8e-a993-4166-a797-b489fabf94f8",
   "metadata": {},
   "source": [
    "### Train, Validation, and Test Averages for each Classifer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "486666b1-9c56-4c2d-8e3f-02ca0bf9bc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Classifier  Train Accuracy Average  \\\n",
      "0     Logistic Regression                0.874777   \n",
      "1          Random Forests                0.909979   \n",
      "2                     SVM                0.894592   \n",
      "3           Decision Tree                0.876710   \n",
      "4  Multi-Layer Perceptron                0.980065   \n",
      "\n",
      "   Cross Validation Accuracy Average  Test Accuracy Average  \n",
      "0                           0.839792               0.844453  \n",
      "1                           0.853165               0.860809  \n",
      "2                           0.865218               0.880037  \n",
      "3                           0.791422               0.790130  \n",
      "4                           0.852472               0.852472  \n"
     ]
    }
   ],
   "source": [
    "train_average_lr = (train_average_lr1 + train_average_lr2 + train_average_lr3 + train_average_lr4) / 4\n",
    "train_average_rf = (train_average_rf1 + train_average_rf2 + train_average_rf3 + train_average_rf4) / 4\n",
    "train_average_svm = (train_average_svm1 + train_average_svm2 + train_average_svm3 + train_average_svm4) / 4\n",
    "train_average_dt = (train_average_dt1 + train_average_dt2 + train_average_dt3 + train_average_dt4) / 4\n",
    "train_average_mlp = (train_average_mlp1 + train_average_mlp2 + train_average_mlp3 + train_average_mlp4) / 4\n",
    "\n",
    "cv_average_lr = (cv_average_lr1 + cv_average_lr2 + cv_average_lr3 + cv_average_lr4) / 4\n",
    "cv_average_rf = (cv_average_rf1 + cv_average_rf2 + cv_average_rf3 + cv_average_lr4) / 4\n",
    "cv_average_svm = (cv_average_svm1 + cv_average_svm2 + cv_average_svm3 + cv_average_lr4) / 4\n",
    "cv_average_dt = (cv_average_dt1 + cv_average_dt2 + cv_average_dt3 + cv_average_lr4) / 4\n",
    "cv_average_mlp = (cv_average_mlp1 + cv_average_mlp2 + cv_average_mlp3 + cv_average_lr4) / 4\n",
    "\n",
    "test_average_lr = (test_average_lr1 + test_average_lr2 + test_average_lr3 + test_average_lr4) / 4\n",
    "test_average_rf = (test_average_rf1 + test_average_rf2 + test_average_rf3 + test_average_lr4) / 4\n",
    "test_average_svm = (test_average_svm1 + test_average_svm2 + test_average_svm3 + test_average_lr4) / 4\n",
    "test_average_dt = (cv_average_dt1 + cv_average_dt2 + cv_average_dt3 + cv_average_dt4) / 4\n",
    "test_average_mlp = (cv_average_mlp1 + cv_average_mlp2 + cv_average_mlp3 + cv_average_lr4) / 4\n",
    "\n",
    "\n",
    "averages = pd.DataFrame({\n",
    "    \"Classifier\" : [\"Logistic Regression\", \"Random Forests\", \"SVM\", \"Decision Tree\", \"Multi-Layer Perceptron\"],\n",
    "    \"Train Accuracy Average\" : [train_average_lr, train_average_rf, train_average_svm, train_average_dt, train_average_mlp], \n",
    "    \"Cross Validation Accuracy Average\" : [cv_average_lr, cv_average_rf, cv_average_svm, cv_average_dt, cv_average_mlp], \n",
    "    \"Test Accuracy Average\" : [test_average_lr, test_average_rf, test_average_svm, test_average_dt, test_average_mlp]\n",
    "})\n",
    "\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7330c-b6c4-4a3a-ac87-bb8b3e9694f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
